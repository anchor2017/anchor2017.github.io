---
layout:     post
title:      python爬虫
subtitle:   
date:       2020-05-12
author:     蔡银锚
header-img:
catalog: true
tags:
    - python
    - 爬虫
    - 正则
    - Scrapy
---


## 爬虫相关

更新pip，如果不成功的话，可以尝试不在pycharm里面更新，直接在系统的cmd上面更新。更新命令：python -m pip install --upgrade pip / pip install --upgrade pip

## Scrapy框架安装

首先需要安装Scrapy框架，如果windows操作系统通过pip的方式安装的话，可能不会成功。第二次在其他机器上面安装发现不管是pip也好还是通过IDE的setting安装也好，都会出现很多错误，我之前好像也没有这么麻烦==，大概要安装4个依赖库。所以，这次的安装下载了anaconda（如果下载anaconda的话，不用下载python解释器，anaconda自带python解释器），居然要下载两个小时左右...官网下载由于服务器在国外，速度很慢。镜像下载：[https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=N&O=D](https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=N&O=D) ，记得关了vpn下载。大概十几分钟。
下载注意：勾选两个勾，添加路径到环境变量。cmd提示激活的解决方法（不激活也没有关系）：[https://www.liaoxuefeng.com/discuss/969955749132672/1276429273948960](https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=N&O=D)
anaconda使用pycharm的方法（在设置中将解释器改为anaconda路径）即可：[https://www.cnblogs.com/yuxuefeng/articles/9235431.html](https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=N&O=D)
anaconda安装方式：[https://cuiqingcai.com/5421.html](https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=N&O=D)

anaconda命令：conda install scrapy


## 新建爬虫项目

（pycharm的terminal直通cmd）
新建爬虫项目cmd: scrapy startproject project_name

## 绑定要爬取的域名

爬虫cmd: scrapy genspider example example.com

## 写好爬虫之后开启爬虫

开启爬虫cmd: scrapy crawl example


## 编写文件
基本的爬虫解析（也就是之后要编写的代码），集中在spiders文件夹下的py文件的特定example爬虫类里面。爬虫类继承scrapy.Spider。

## 引入的包
```python
import scrapy
import re
```

## 爬虫类全局变量
```python
# 爬虫的名字、允许爬取网页的域名（起过滤作用）、开始爬取也就是第一个爬取的页面地址。
name = ''
allowed_domains = []
start_urls = []
```

## 解析函数
```python
parse(self, response) # 第一个参数是类自身、第二个参数是返回的页面
# 使用re解析方式
ids = response.selector.re('.*<b class="fl"><a href="/Product/(.*?)">.*</a></b>.*')
# 两个列表合成一个字典
for x, y in zip(ids, titles):
    id_title[x] = y
# 继续发起爬虫请求
yield scrapy.Request(new_url, self.parse_url)
# 单纯调用re模块，字符串前面加个字母r，第一个参数是模板、第二个参数是要匹配的字符串，结果返回的是一个列表，括号中的(.*?)即为列表的第一个元素，也可以使用(.+?)表示匹配至少一个字符。
f_id = re.findall(r".*qClsId=(.*?)/", response.url+'/')[0]
# 如果要附带额外参数可以使用meta,meta字典要放在最后面。
def parse_url(self, response):
    ...
    yield scrapy.Request(last_url, self.parse_last_url, meta={'f_id': f_id})
def parse_last_url(self, response):
    f_id = response.meta['f_id']
    ...

# 匹配换行符，可以使用[\s\S]*，.*可以匹配除了换行符之外的其他字符。
text2 = response.selector.re('.*<div class="product_content clearfix  c-33">([\s\S]*)<div class="product_info_right fr  bsd">.*')[0]

# 去掉html文件中的<xxxxx>双尖括号和里面的内容
dr = re.compile(r'<[^>]+>', re.S)
dd = dr.sub('', text2)
dd = ''.join(dd.split())

# 写入文件跳过无法匹配的字符，errors='ignore'
with open(file_pre+'.txt', 'a', errors='ignore') as f:
    f.write(title+'\n')
```




















